{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single and Multi Headed attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query, key, value vectors are not identical in practice.\n",
    "- They are each a linear transformation of embedding vector.\n",
    "- This allows self attention layer to focus on complementary words in the context rather than the same word itself.\n",
    "- Linear transformation is done with learnable parameters.\n",
    "- In single headed attn, only transformation is created for each of query, key and value\n",
    "- In multi headed attn, multiple transformations are created.\n",
    "- Each head encode a different aspect of similarity like subject-verb interaction, nearby adjectives etc.\n",
    "- These as aspects are fully learned from data and are similar to filters in CNNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single headed attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "from torch import nn, bmm\n",
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_prod_attn(query, key, value):\n",
    "\n",
    "    dim_k = query.size(-1)\n",
    "    scores = bmm(query, key.transpose(1,2))/sqrt(dim_k)\n",
    "    weights = nn.functional.softmax(scores, dim=-1)\n",
    "    attn_outputs = bmm(weights, value)\n",
    "    return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One attention head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embed_dim:\n",
    "\n",
    "head_dim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(self.q(hidden_state),\n",
    "                                                    self.k(hidden_state),\n",
    "                                                    self.v(hidden_state))\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddab157190c3c29c7bfa9724dd2612e80e7d4a281bb7f76e54f36d2e23abd8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
